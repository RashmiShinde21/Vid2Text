TrainingArguments:
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 1 # increase by 2x for every 2x decrease in batch size
  learning_rate: 1e-5
  lr_scheduler_type: "constant_with_warmup"
  warmup_steps: 50
  max_steps: 1 # increase to 4000 if you have your own GPU or a Colab paid plan
  gradient_checkpointing: True
  evaluation_strategy: "steps"
  per_device_eval_batch_size: 5
  predict_with_generate: True
  generation_max_length: 225
  save_steps: 1
  eval_steps: 1
  logging_steps: 1
  report_to: ["tensorboard"]
  load_best_model_at_end: True
  metric_for_best_model: "wer"
  greater_is_better: False
